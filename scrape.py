import os
import time
import re
import requests
from bs4 import BeautifulSoup
from bs4 import Comment
from urllib.parse import urljoin, urlparse, urlunparse
from io import BytesIO
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
import fitz  # PyMuPDF
from docx import Document
import csv

# === è¨­å®š ===
ENABLE_OCR = False  # ç”»åƒOCRã‚’ONã«ã™ã‚‹å ´åˆ True, OFFã«ã™ã‚‹å ´åˆ False
ENABLE_PDF = False   # Trueãªã‚‰PDFã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º

# é™¤å¤–ã—ãŸã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
exclude_paths = [
    '/en', '/pressroom',  # ä¾‹ï¼‰/en/ é…ä¸‹ã‚„ pressroom é…ä¸‹ã¯é™¤å¤–
]

# å¯¾è±¡ã«ã—ãŸã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆç©ºãƒªã‚¹ãƒˆãªã‚‰å…¨ä½“å¯¾è±¡ï¼‰
include_only_prefix = []

# OCRç”¨ è¨€èªè¨­å®šï¼ˆæ—¥æœ¬èªï¼‹è‹±èªï¼‰
ocr_lang = 'jpn+eng'

# --- ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼†è£œåŠ©é–¢æ•° ---
def sanitize_text(text):
    return re.sub(r'[\\/*?:"<>|]', "_", text)

def normalize_url(url):
    parsed = urlparse(url)
    path = parsed.path.rstrip('/')
    normalized = urlunparse((parsed.scheme, parsed.netloc, path, '', '', ''))
    return normalized

def clean_text(text):
    if not text:
        return ''
    text = text.replace('\x00', '')
    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')

    def is_valid_xml_char(c):
        codepoint = ord(c)
        return (
            codepoint == 0x9 or
            codepoint == 0xA or
            codepoint == 0xD or
            (0x20 <= codepoint <= 0xD7FF) or
            (0xE000 <= codepoint <= 0xFFFD) or
            (0x10000 <= codepoint <= 0x10FFFF)
        )

    text = ''.join(c for c in text if is_valid_xml_char(c))
    return text

def should_visit(url):
    path = urlparse(url).path
    for exclude in exclude_paths:
        if path.startswith(exclude):
            return False
    if include_only_prefix:
        return any(path.startswith(prefix) for prefix in include_only_prefix)
    return True

def scrape_website(start_url, output_file, exclude_paths=None, enable_ocr=False, enable_pdf=False, include_only_prefix=None, progress_callback=None):
    if exclude_paths is None:
        exclude_paths = []
    if include_only_prefix is None:
        include_only_prefix = []
    visited = set()
    to_visit = [start_url]
    domain = urlparse(start_url).netloc
    doc = Document()
    is_first_page = True
    processed_pages = 0

    def should_visit(url):
        path = urlparse(url).path
        for exclude in exclude_paths:
            if path.startswith(exclude):
                return False
        if include_only_prefix:
            return any(path.startswith(prefix) for prefix in include_only_prefix)
        return True

    while to_visit:
        url = to_visit.pop()
        normalized_current_url = normalize_url(url)
        if normalized_current_url in visited:
            continue
        visited.add(normalized_current_url)

        try:
            response = requests.get(url)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            continue

        soup = BeautifulSoup(response.text, 'html.parser')

        # --- <header>ã‚¿ã‚°ã¨ãã®é–¢é€£è¦ç´ ã‚’å®Œå…¨ã«é™¤å» ---
        # headerã‚¿ã‚°è‡ªä½“ã‚’é™¤å»
        for header_tag in soup.find_all('header'):
            header_tag.decompose()
        
        # headerã«é–¢é€£ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚„IDã‚’æŒã¤è¦ç´ ã‚‚é™¤å»
        header_selectors = [
            {'class_': ['header', 'site-header', 'main-header', 'global-header']},
            {'id': ['header', 'site-header', 'main-header', 'global-header']},
            {'role': 'banner'},
            {'class_': ['header-nav', 'header-menu', 'header-wrapper']},
            {'id': ['header-nav', 'header-menu', 'header-wrapper']}
        ]
        
        for selector in header_selectors:
            for tag in soup.find_all(attrs=selector):
                tag.decompose()

        # --- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³é™¤å» ---
        for nav in soup.find_all('nav'):
            nav.decompose()
        # <header>ã‚¿ã‚°å†…ã®ãƒªã‚¹ãƒˆã‚„ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã¯æ—¢ã«é™¤å»ã•ã‚Œã‚‹

        unwanted_selectors = [
            {'id': 'global-nav'},
            {'id': 'gnav'},
            {'class': 'global-nav'},
            {'class': 'header-nav'},
            {'class': 'menu'},
            {'class': 'nav'},
            {'id': 'header'},
            {'id': 'footer'},
        ]
        for selector in unwanted_selectors:
            for tag in soup.find_all(attrs=selector):
                tag.decompose()

        # â˜… ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã ã‘å…ˆã«å–å¾—
        description_text = None
        description_tag = soup.find('meta', attrs={'name': 'description'})
        if description_tag and description_tag.get('content'):
            description_text = description_tag.get('content').strip()

        # â˜… ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å‰Šé™¤
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()

        # â˜… ä¸è¦ãªmeta, script, style, noscript, iframe, link, svgã‚¿ã‚°ã‚’å‰Šé™¤
        for tag in soup(['meta', 'script', 'style', 'noscript', 'iframe', 'link', 'svg']):
            tag.decompose()

        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "no_title"
        clean_title = sanitize_text(title)

        # ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šï¼ˆæœ€åˆã®1ãƒšãƒ¼ã‚¸ç›®ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not is_first_page:
            doc.add_page_break()
        else:
            is_first_page = False
        doc.add_heading(clean_text(clean_title), level=1)

        # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°å‡ºåŠ›
        if description_text:
            doc.add_paragraph(clean_text(f"ã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã€‘\n{description_text}"))

        # æœ¬æ–‡ã‚¨ãƒªã‚¢
        main_content = soup.find('main')
        if main_content:
            content_source = main_content
        else:
            content_source = soup

        # æœ¬æ–‡ã®ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        for element in content_source.descendants:
            if element.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                text = element.get_text(strip=True)
                if text:
                    level = int(element.name[1])
                    doc.add_heading(clean_text(text), level=level)
            elif element.name == 'li':
                text = element.get_text(strip=True)
                if text:
                    doc.add_paragraph(clean_text(text), style='List Bullet')
            elif element.name is None:
                text = element.strip()
                if text:
                    doc.add_paragraph(clean_text(text))

        # ç”»åƒã‹ã‚‰OCRæŠ½å‡º (ON/OFF)
        if enable_ocr and OCR_AVAILABLE:
            ocr_texts = []
            for img_tag in soup.find_all('img'):
                img_url = img_tag.get('src')
                if img_url:
                    img_full_url = urljoin(url, img_url)
                    if img_full_url.lower().endswith('.svg'):
                        continue
                    try:
                        img_response = requests.get(img_full_url)
                        img_response.raise_for_status()
                        img = Image.open(BytesIO(img_response.content))
                        text_from_image = pytesseract.image_to_string(img, lang=ocr_lang).strip()
                        if text_from_image:
                            ocr_texts.append(text_from_image)
                    except Exception as e:
                        print(f"ç”»åƒã‚¨ãƒ©ãƒ¼: {img_full_url} - {e}")
                        continue
            if ocr_texts:
                doc.add_heading("ç”»åƒã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ", level=2)
                for ocr_text in ocr_texts:
                    doc.add_paragraph(clean_text(ocr_text))
        elif enable_ocr and not OCR_AVAILABLE:
            doc.add_paragraph("â€» OCRæ©Ÿèƒ½ã¯ç¾åœ¨ã®ç’°å¢ƒã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")

        # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º (ON/OFF)
        if enable_pdf:
            for link_tag in soup.find_all('a', href=True):
                href = link_tag['href']
                if href.lower().endswith('.pdf'):
                    full_url = urljoin(url, href)
                    try:
                        pdf_response = requests.get(full_url)
                        pdf_response.raise_for_status()
                        pdf_bytes = BytesIO(pdf_response.content)
                        doc_pdf = fitz.open(stream=pdf_bytes, filetype="pdf")
                        extracted_text = ""
                        for page in doc_pdf:
                            extracted_text += page.get_text()
                        if extracted_text.strip():
                            doc.add_heading("PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ", level=2)
                            doc.add_paragraph(clean_text(extracted_text.strip()))
                    except Exception as e:
                        print(f"PDFã‚¨ãƒ©ãƒ¼: {full_url} - {e}")
                        continue

        # === ğŸ†• ãƒªãƒ³ã‚¯åé›† ===
        for link_tag in soup.find_all('a', href=True):
            href = link_tag['href']
            full_url = urljoin(url, href)
            normalized_url = normalize_url(full_url)
            if (
                urlparse(normalized_url).netloc == domain and
                normalized_url not in visited and
                should_visit(normalized_url) and
                not normalized_url.lower().endswith('.pdf')
            ):
                to_visit.append(normalized_url)

        # â˜… ãƒšãƒ¼ã‚¸ã”ã¨ã«é€²æ—ã‚’è¡¨ç¤ºãƒ»å ±å‘Š
        processed_pages += 1
        current_total = processed_pages + len(to_visit)
        if progress_callback:
            progress_callback(done=processed_pages, total=current_total)
        
        path = urlparse(url).path
        print(f"æ›¸ãè¾¼ã¿å®Œäº†: {path} - {clean_title}")
        time.sleep(1)

    doc.save(output_file)
    print(f"å…¨ãƒšãƒ¼ã‚¸ã‚’ {output_file} ã«ã¾ã¨ã‚ã¾ã—ãŸï¼")
    return output_file

def list_all_urls(start_url, exclude_paths=None, include_only_prefix=None, progress_callback=None):
    if exclude_paths is None:
        exclude_paths = []
    if include_only_prefix is None:
        include_only_prefix = []
    visited = set()
    to_visit = [start_url]
    domain = urlparse(start_url).netloc
    url_list = []
    total = 0

    def should_visit(url):
        path = urlparse(url).path
        for exclude in exclude_paths:
            if path.startswith(exclude):
                return False
        if include_only_prefix:
            return any(path.startswith(prefix) for prefix in include_only_prefix)
        return True

    while to_visit:
        url = to_visit.pop()
        normalized_current_url = normalize_url(url)
        if normalized_current_url in visited:
            continue
        visited.add(normalized_current_url)
        url_list.append(normalized_current_url)
        total += 1
        if progress_callback:
            progress_callback(done=total, total=len(visited) + len(to_visit))
        try:
            response = requests.get(url)
            response.raise_for_status()
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            continue
        soup = BeautifulSoup(response.text, 'html.parser')
        for link_tag in soup.find_all('a', href=True):
            href = link_tag['href']
            full_url = urljoin(url, href)
            normalized_url = normalize_url(full_url)
            if (
                urlparse(normalized_url).netloc == domain and
                normalized_url not in visited and
                should_visit(normalized_url) and
                not normalized_url.lower().endswith('.pdf')
            ):
                to_visit.append(normalized_url)
    return url_list

def list_all_urls_with_stats(start_url, output_file, exclude_paths=None, include_only_prefix=None, progress_callback=None):
    if exclude_paths is None:
        exclude_paths = []
    if include_only_prefix is None:
        include_only_prefix = []
    visited = set()
    to_visit = [start_url]
    domain = urlparse(start_url).netloc
    url_rows = []  # å„ãƒšãƒ¼ã‚¸ã®æƒ…å ±
    dir_stats = {}  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã®çµ±è¨ˆ {'/service/': {'page': 0, 'pdf': 0}, ...}
    total_pages = 0
    total_pdfs = 0

    def should_visit(url):
        path = urlparse(url).path
        for exclude in exclude_paths:
            if path.startswith(exclude):
                return False
        if include_only_prefix:
            return any(path.startswith(prefix) for prefix in include_only_prefix)
        return True

    def get_directory(path):
        # ä¾‹: /service/abc â†’ /service/
        if not path or path == '/':
            return '/'
        parts = path.strip('/').split('/')
        return '/' + parts[0] + '/'

    while to_visit:
        url = to_visit.pop()
        normalized_current_url = normalize_url(url)
        if normalized_current_url in visited:
            continue
        visited.add(normalized_current_url)
        path = urlparse(normalized_current_url).path
        directory = get_directory(path)
        is_pdf = 1 if normalized_current_url.lower().endswith('.pdf') else 0
        url_rows.append([normalized_current_url, directory, is_pdf])
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆ
        if directory not in dir_stats:
            dir_stats[directory] = {'page': 0, 'pdf': 0}
        dir_stats[directory]['page'] += 1
        if is_pdf:
            dir_stats[directory]['pdf'] += 1
            total_pdfs += 1
        total_pages += 1
        # é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå‡¦ç†æ¸ˆã¿ä»¶æ•°ã¨æ®‹ã‚Šä»¶æ•°ã®åˆè¨ˆã§è¨ˆç®—ï¼‰
        if progress_callback:
            current_total = total_pages + len(to_visit)
            progress_callback(done=total_pages, total=current_total)
        try:
            response = requests.get(url)
            response.raise_for_status()
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {url} - {e}")
            continue
        soup = BeautifulSoup(response.text, 'html.parser')
        for link_tag in soup.find_all('a', href=True):
            href = link_tag['href']
            full_url = urljoin(url, href)
            normalized_url = normalize_url(full_url)
            if (
                urlparse(normalized_url).netloc == domain and
                normalized_url not in visited and
                should_visit(normalized_url)
            ):
                to_visit.append(normalized_url)
    # CSVå‡ºåŠ›
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        # ãƒšãƒ¼ã‚¸ä¸€è¦§
        writer.writerow(['url', 'directory', 'is_pdf'])
        for row in url_rows:
            writer.writerow(row)
        # ç©ºè¡Œ
        writer.writerow([])
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªçµ±è¨ˆ
        writer.writerow(['directory', 'page_count', 'pdf_count'])
        for directory, stats in dir_stats.items():
            writer.writerow([directory, stats['page'], stats['pdf']])
        # ç©ºè¡Œ
        writer.writerow([])
        # ã‚µã‚¤ãƒˆå…¨ä½“çµ±è¨ˆ
        writer.writerow(['total_pages', 'total_pdfs'])
        writer.writerow([total_pages, total_pdfs])
    return output_file